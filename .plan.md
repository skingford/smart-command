# AI Integration Enhancement Plan

## Requirements Restatement

为 smart-command 增强 AI 集成功能：
1. **提供各大模型供应商的详细配置案例** - 用户可以参照配置快速接入
2. **支持在 REPL 中动态切换 AI 供应商** - 无需重启即可切换
3. **改进配置结构** - 支持预配置多个供应商，快速切换
4. **添加供应商管理命令** - `ai list`, `ai use <provider>`, `ai status`

---

## Implementation Phases

### Phase 1: Enhanced Configuration Structure

#### 1.1 Multi-Provider Configuration

重新设计 `AiConfig` 支持多供应商预配置：

```rust
// src/config.rs

/// AI completion configuration
#[derive(Debug, Deserialize, Clone)]
pub struct AiConfig {
    /// Enable AI completion (Alt+L)
    #[serde(default)]
    pub enabled: bool,

    /// Currently active provider name
    #[serde(default = "default_ai_provider")]
    pub active: String,

    /// Pre-configured providers
    #[serde(default)]
    pub providers: HashMap<String, ProviderConfig>,

    /// Global settings (can be overridden per provider)
    #[serde(default)]
    pub global: GlobalAiSettings,
}

#[derive(Debug, Deserialize, Clone)]
pub struct ProviderConfig {
    /// Provider type: claude, openai, gemini, glm, deepseek, ollama, custom
    pub provider_type: String,

    /// API key (use $ENV_VAR reference)
    #[serde(default)]
    pub api_key: Option<String>,

    /// API endpoint (required for custom, ollama)
    #[serde(default)]
    pub endpoint: Option<String>,

    /// Model name
    #[serde(default)]
    pub model: Option<String>,

    /// Override global settings
    #[serde(default)]
    pub max_tokens: Option<u32>,

    #[serde(default)]
    pub temperature: Option<f32>,
}

#[derive(Debug, Deserialize, Clone)]
pub struct GlobalAiSettings {
    /// System prompt for command generation
    #[serde(default = "default_ai_system_prompt")]
    pub system_prompt: String,

    /// Max tokens for response
    #[serde(default = "default_ai_max_tokens")]
    pub max_tokens: u32,

    /// Temperature for generation
    #[serde(default = "default_ai_temperature")]
    pub temperature: f32,

    /// Request timeout in seconds
    #[serde(default = "default_ai_timeout")]
    pub timeout_secs: u64,
}
```

#### 1.2 Configuration Examples

完整的配置文件示例 (`~/.config/smart-command/config.toml`):

```toml
[ai]
enabled = true
active = "claude"  # Current active provider

[ai.global]
system_prompt = """
You are a shell command expert. Generate a single shell command based on the user's natural language description.
Rules:
- Output ONLY the command, no explanations
- Use common Unix/Linux commands
- Prefer portable solutions
- If multiple commands needed, use && or pipes
- Never use dangerous commands without explicit request
"""
max_tokens = 256
temperature = 0.3
timeout_secs = 30

# ==================== Provider Configurations ====================

# Claude (Anthropic) - Recommended
[ai.providers.claude]
provider_type = "claude"
api_key = "$ANTHROPIC_API_KEY"
model = "claude-3-5-sonnet-20241022"
# Alternative models:
# model = "claude-3-5-haiku-20241022"   # Faster, cheaper
# model = "claude-3-opus-20240229"      # Most capable

# OpenAI
[ai.providers.openai]
provider_type = "openai"
api_key = "$OPENAI_API_KEY"
model = "gpt-4o"
# Alternative models:
# model = "gpt-4o-mini"                 # Faster, cheaper
# model = "gpt-4-turbo"                 # Previous generation

# Google Gemini
[ai.providers.gemini]
provider_type = "gemini"
api_key = "$GOOGLE_API_KEY"
model = "gemini-1.5-flash"
# Alternative models:
# model = "gemini-1.5-pro"              # More capable
# model = "gemini-2.0-flash-exp"        # Experimental

# DeepSeek (Chinese provider, very cost effective)
[ai.providers.deepseek]
provider_type = "deepseek"
api_key = "$DEEPSEEK_API_KEY"
model = "deepseek-chat"
endpoint = "https://api.deepseek.com/v1/chat/completions"

# 智谱 GLM (ZhiPu AI)
[ai.providers.glm]
provider_type = "glm"
api_key = "$ZHIPU_API_KEY"
model = "glm-4-flash"
# Alternative models:
# model = "glm-4"                       # More capable
# model = "glm-4-plus"                  # Most capable

# 通义千问 (Alibaba Qwen)
[ai.providers.qwen]
provider_type = "qwen"
api_key = "$DASHSCOPE_API_KEY"
model = "qwen-turbo"
endpoint = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
# Alternative models:
# model = "qwen-plus"
# model = "qwen-max"

# 文心一言 (Baidu ERNIE)
[ai.providers.ernie]
provider_type = "ernie"
api_key = "$ERNIE_API_KEY"  # Format: api_key:secret_key
model = "ernie-4.0-8k"
endpoint = "https://aip.baidubce.com/rpc/2.0/ai_custom/v1/wenxinworkshop/chat/completions_pro"

# Ollama (Local, free)
[ai.providers.ollama]
provider_type = "ollama"
endpoint = "http://localhost:11434/api/generate"
model = "llama3.2"
# Alternative models (depends on what you've pulled):
# model = "codellama"
# model = "mistral"
# model = "qwen2.5-coder"

# OpenRouter (Multi-model gateway)
[ai.providers.openrouter]
provider_type = "openrouter"
api_key = "$OPENROUTER_API_KEY"
endpoint = "https://openrouter.ai/api/v1/chat/completions"
model = "anthropic/claude-3.5-sonnet"
# Access many models through one API:
# model = "openai/gpt-4o"
# model = "google/gemini-pro-1.5"
# model = "meta-llama/llama-3.1-70b-instruct"

# Custom OpenAI-compatible API
[ai.providers.custom]
provider_type = "custom"
api_key = "$CUSTOM_API_KEY"
endpoint = "https://your-api.example.com/v1/chat/completions"
model = "your-model-name"
```

### Phase 2: Provider Management Commands

#### 2.1 REPL Commands

添加 AI 管理命令到 REPL：

```
ai                   - Show current AI status and active provider
ai list              - List all configured providers
ai use <provider>    - Switch to a different provider
ai test              - Test current provider with a simple query
ai providers         - Show supported provider types with setup info
```

#### 2.2 Implementation in main.rs

```rust
// Handle `ai` commands
if trimmed == "ai" || trimmed.starts_with("ai ") {
    let parts: Vec<&str> = trimmed.split_whitespace().collect();

    match parts.get(1).map(|s| *s) {
        None | Some("status") => {
            // Show current AI status
            if config.ai.enabled {
                Output::info(&format!("AI Status: Enabled"));
                Output::info(&format!("Active Provider: {}", config.ai.active));
                if let Some(provider) = config.ai.providers.get(&config.ai.active) {
                    Output::dim(&format!("  Type: {}", provider.provider_type));
                    Output::dim(&format!("  Model: {}", provider.model.as_deref().unwrap_or("default")));
                }
            } else {
                Output::warn("AI Status: Disabled");
                Output::dim("Enable with: ai.enabled = true in config.toml");
            }
        }
        Some("list") => {
            // List configured providers
            Output::info("Configured AI Providers:");
            for (name, provider) in &config.ai.providers {
                let active = if name == &config.ai.active { " (active)" } else { "" };
                println!("  {} - {}{}", name, provider.provider_type, active);
            }
        }
        Some("use") => {
            if let Some(provider_name) = parts.get(2) {
                if config.ai.providers.contains_key(*provider_name) {
                    config.ai.active = provider_name.to_string();
                    Output::success(&format!("Switched to: {}", provider_name));
                } else {
                    Output::error(&format!("Unknown provider: {}", provider_name));
                    Output::dim("Use 'ai list' to see available providers");
                }
            } else {
                Output::error("Usage: ai use <provider>");
            }
        }
        Some("test") => {
            // Test current provider
            Output::info("Testing AI provider...");
            let generator = ai::llm::AiCommandGenerator::new(&config.ai);
            match generator.generate("list files in current directory", &ai::llm::AiContext::default()) {
                Ok(cmd) => Output::success(&format!("Test passed! Generated: {}", cmd)),
                Err(e) => Output::error(&format!("Test failed: {}", e)),
            }
        }
        Some("providers") => {
            // Show supported provider types
            println!("Supported AI Provider Types:");
            println!();
            println!("  claude     - Anthropic Claude (recommended)");
            println!("              API Key: ANTHROPIC_API_KEY");
            println!("              Models: claude-3-5-sonnet-20241022, claude-3-5-haiku-20241022");
            println!();
            println!("  openai     - OpenAI GPT");
            println!("              API Key: OPENAI_API_KEY");
            println!("              Models: gpt-4o, gpt-4o-mini, gpt-4-turbo");
            println!();
            println!("  gemini     - Google Gemini");
            println!("              API Key: GOOGLE_API_KEY");
            println!("              Models: gemini-1.5-flash, gemini-1.5-pro");
            println!();
            println!("  deepseek   - DeepSeek (cost effective)");
            println!("              API Key: DEEPSEEK_API_KEY");
            println!("              Models: deepseek-chat, deepseek-coder");
            println!();
            println!("  glm        - ZhiPu GLM (智谱)");
            println!("              API Key: ZHIPU_API_KEY");
            println!("              Models: glm-4-flash, glm-4, glm-4-plus");
            println!();
            println!("  qwen       - Alibaba Qwen (通义千问)");
            println!("              API Key: DASHSCOPE_API_KEY");
            println!("              Models: qwen-turbo, qwen-plus, qwen-max");
            println!();
            println!("  ollama     - Ollama (local, free)");
            println!("              No API key needed, runs locally");
            println!("              Models: llama3.2, codellama, mistral, qwen2.5-coder");
            println!();
            println!("  openrouter - OpenRouter (multi-model gateway)");
            println!("              API Key: OPENROUTER_API_KEY");
            println!("              Access 100+ models through one API");
            println!();
            println!("  custom     - Custom OpenAI-compatible API");
            println!("              Configure your own endpoint");
        }
        _ => {
            Output::error("Unknown ai command");
            Output::dim("Available: ai, ai list, ai use <provider>, ai test, ai providers");
        }
    }
    continue;
}
```

### Phase 3: Add More Provider Implementations

#### 3.1 DeepSeek Provider

```rust
fn call_deepseek(&self, api_key: &str, user_prompt: &str) -> Result<String, AiError> {
    // DeepSeek uses OpenAI-compatible format
    let model = self.config.model.clone()
        .unwrap_or_else(|| "deepseek-chat".to_string());

    let endpoint = self.config.endpoint.clone()
        .unwrap_or_else(|| "https://api.deepseek.com/v1/chat/completions".to_string());

    self.call_openai_compatible(&api_key, &user_prompt, &endpoint, &model)
}
```

#### 3.2 Qwen (Alibaba) Provider

```rust
fn call_qwen(&self, api_key: &str, user_prompt: &str) -> Result<String, AiError> {
    let model = self.config.model.clone()
        .unwrap_or_else(|| "qwen-turbo".to_string());

    let endpoint = self.config.endpoint.clone()
        .unwrap_or_else(|| "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions".to_string());

    self.call_openai_compatible(&api_key, &user_prompt, &endpoint, &model)
}
```

#### 3.3 Ollama (Local) Provider

```rust
fn call_ollama(&self, _api_key: &str, user_prompt: &str) -> Result<String, AiError> {
    // Ollama has a different API format
    #[derive(Serialize)]
    struct OllamaRequest {
        model: String,
        prompt: String,
        stream: bool,
    }

    #[derive(Deserialize)]
    struct OllamaResponse {
        response: String,
    }

    let model = self.config.model.clone()
        .unwrap_or_else(|| "llama3.2".to_string());

    let endpoint = self.config.endpoint.clone()
        .unwrap_or_else(|| "http://localhost:11434/api/generate".to_string());

    let full_prompt = format!("{}\n\n{}", self.config.system_prompt, user_prompt);

    let request = OllamaRequest {
        model,
        prompt: full_prompt,
        stream: false,
    };

    let response = self.client
        .post(&endpoint)
        .header("content-type", "application/json")
        .json(&request)
        .send()
        .map_err(|e| AiError::NetworkError(e.to_string()))?;

    if !response.status().is_success() {
        let status = response.status();
        let body = response.text().unwrap_or_default();
        return Err(AiError::ApiError(format!("Status {}: {}", status, body)));
    }

    let result: OllamaResponse = response.json()
        .map_err(|e| AiError::ParseError(e.to_string()))?;

    Ok(result.response.trim().to_string())
}
```

#### 3.4 OpenRouter Provider

```rust
fn call_openrouter(&self, api_key: &str, user_prompt: &str) -> Result<String, AiError> {
    // OpenRouter uses OpenAI-compatible format with extra headers
    let model = self.config.model.clone()
        .unwrap_or_else(|| "anthropic/claude-3.5-sonnet".to_string());

    let endpoint = "https://openrouter.ai/api/v1/chat/completions";

    let request = OpenAiRequest {
        model,
        messages: vec![
            OpenAiMessage {
                role: "system".to_string(),
                content: self.config.system_prompt.clone(),
            },
            OpenAiMessage {
                role: "user".to_string(),
                content: user_prompt.to_string(),
            },
        ],
        max_tokens: self.config.max_tokens,
        temperature: self.config.temperature,
    };

    let response = self.client
        .post(endpoint)
        .header("Authorization", format!("Bearer {}", api_key))
        .header("HTTP-Referer", "https://github.com/skingford/smart-command")
        .header("X-Title", "Smart Command Shell")
        .header("content-type", "application/json")
        .json(&request)
        .send()
        .map_err(|e| AiError::NetworkError(e.to_string()))?;

    // ... standard OpenAI response parsing
}
```

### Phase 4: Refactor Common Code

#### 4.1 Extract OpenAI-Compatible Helper

```rust
/// Call any OpenAI-compatible API endpoint
fn call_openai_compatible(
    &self,
    api_key: &str,
    user_prompt: &str,
    endpoint: &str,
    model: &str,
) -> Result<String, AiError> {
    let request = OpenAiRequest {
        model: model.to_string(),
        messages: vec![
            OpenAiMessage {
                role: "system".to_string(),
                content: self.config.system_prompt.clone(),
            },
            OpenAiMessage {
                role: "user".to_string(),
                content: user_prompt.to_string(),
            },
        ],
        max_tokens: self.config.max_tokens,
        temperature: self.config.temperature,
    };

    let response = self.client
        .post(endpoint)
        .header("Authorization", format!("Bearer {}", api_key))
        .header("content-type", "application/json")
        .json(&request)
        .send()
        .map_err(|e| AiError::NetworkError(e.to_string()))?;

    if !response.status().is_success() {
        let status = response.status();
        let body = response.text().unwrap_or_default();
        return Err(AiError::ApiError(format!("Status {}: {}", status, body)));
    }

    let result: OpenAiResponse = response.json()
        .map_err(|e| AiError::ParseError(e.to_string()))?;

    result.choices.first()
        .map(|c| c.message.content.trim().to_string())
        .ok_or_else(|| AiError::ParseError("Empty response".to_string()))
}
```

---

## File Changes Summary

### Modified Files:
1. `src/config.rs` - Multi-provider configuration structure
2. `src/ai.rs` - Add providers: deepseek, qwen, ollama, openrouter; refactor common code
3. `src/main.rs` - Add `ai` command handlers for REPL

### New Documentation:
1. Update `README.md` with AI configuration examples
2. Update example config in `config.rs`

---

## API Key Environment Variables Reference

| Provider | Environment Variable | How to Get |
|----------|---------------------|------------|
| Claude | `ANTHROPIC_API_KEY` | https://console.anthropic.com/settings/keys |
| OpenAI | `OPENAI_API_KEY` | https://platform.openai.com/api-keys |
| Gemini | `GOOGLE_API_KEY` | https://aistudio.google.com/app/apikey |
| DeepSeek | `DEEPSEEK_API_KEY` | https://platform.deepseek.com/api_keys |
| GLM | `ZHIPU_API_KEY` | https://open.bigmodel.cn/usercenter/apikeys |
| Qwen | `DASHSCOPE_API_KEY` | https://dashscope.console.aliyun.com/apiKey |
| OpenRouter | `OPENROUTER_API_KEY` | https://openrouter.ai/keys |
| Ollama | (none needed) | Run locally: `ollama serve` |

---

## Estimated Complexity

| Phase | Complexity | Description |
|-------|------------|-------------|
| Phase 1 | MEDIUM | Config structure refactoring |
| Phase 2 | MEDIUM | REPL command handlers |
| Phase 3 | LOW | Add provider implementations (similar patterns) |
| Phase 4 | LOW | Code refactoring |

---

## Risks

1. **Breaking Change**: Config format changes may affect existing users
   - Mitigation: Support both old and new config formats during transition

2. **API Compatibility**: Different providers may have subtle API differences
   - Mitigation: Comprehensive testing with each provider

3. **Rate Limits**: Users may hit rate limits with rapid switching
   - Mitigation: Add rate limiting and clear error messages

---

## Acceptance Criteria

- [ ] Multi-provider configuration supported in config.toml
- [ ] `ai list` shows all configured providers
- [ ] `ai use <provider>` switches active provider without restart
- [ ] `ai test` validates current provider connection
- [ ] `ai providers` shows setup info for all supported providers
- [ ] DeepSeek, Qwen, Ollama, OpenRouter providers implemented
- [ ] Common OpenAI-compatible code refactored
- [ ] README updated with configuration examples
